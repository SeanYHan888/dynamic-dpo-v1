{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fail Rate From Running Logs\n",
        "\n",
        "This notebook scans your training stdout/stderr logs (or W&B `wandb-history.jsonl`) and computes fail rate from the `risk/fail` metric.\n",
        "\n",
        "By default, it treats `risk/fail == 1` as a failure (you can change `FAIL_VALUE`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146f45d7",
      "metadata": {},
      "source": [
        "## Plot Recommendation\n",
        "\n",
        "- Rolling fail rate over time: line plot of a moving average of `risk/fail` (best for seeing stability/regressions during training).\n",
        "- Per-run/per-file comparison: bar chart of fail rate grouped by log file (best for comparing datasets/experiments).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff114af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Iterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Treat this value of `risk/fail` as a \"failure\".\n",
        "# You asked for failure based on `risk/fail: 0`, so the default is 0.\n",
        "# If in your runs `risk/fail: 1` means failure, set FAIL_VALUE = 1.\n",
        "FAIL_VALUE = 0\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class FailRecord:\n",
        "    path: str\n",
        "    line_no: int\n",
        "    value: int\n",
        "    line: str\n",
        "\n",
        "\n",
        "_RISK_FAIL_RE = re.compile(r\"['\\\"]risk/fail['\\\"]\\s*[:=]\\s*([-+]?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "\n",
        "def _iter_text_files(paths: Iterable[str]) -> Iterator[tuple[str, Iterator[str]]]:\n",
        "    \"\"\"Yield (path, line_iterator) for existing files.\"\"\"\n",
        "    for p in paths:\n",
        "        if not p:\n",
        "            continue\n",
        "        if os.path.isdir(p):\n",
        "            continue\n",
        "        if not os.path.exists(p):\n",
        "            continue\n",
        "        yield p, Path(p).open(\"r\", encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "\n",
        "def _extract_risk_fail_value(line: str) -> int | None:\n",
        "    \"\"\"Extract risk/fail from one line.\n",
        "\n",
        "    Supports:\n",
        "    - JSONL like: {\"risk/fail\": 0, ...}\n",
        "    - Python-ish dict printing like: {'risk/fail': 0, ...}\n",
        "    - Other text like: risk/fail: 0\n",
        "    \"\"\"\n",
        "    # Fast path: regex\n",
        "    m = _RISK_FAIL_RE.search(line)\n",
        "    if m:\n",
        "        try:\n",
        "            return int(float(m.group(1)))\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # JSON path (W&B history lines often are valid JSON)\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
        "        try:\n",
        "            obj = json.loads(s)\n",
        "        except Exception:\n",
        "            return None\n",
        "        if isinstance(obj, dict) and \"risk/fail\" in obj:\n",
        "            try:\n",
        "                return int(float(obj[\"risk/fail\"]))\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_risk_fail_from_logs(paths: Iterable[str]) -> list[FailRecord]:\n",
        "    records: list[FailRecord] = []\n",
        "    for path, f in _iter_text_files(paths):\n",
        "        with f:\n",
        "            for i, line in enumerate(f, start=1):\n",
        "                v = _extract_risk_fail_value(line)\n",
        "                if v is None:\n",
        "                    continue\n",
        "                records.append(FailRecord(path=path, line_no=i, value=v, line=line.rstrip(\"\\n\")))\n",
        "    return records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Point this at your captured stdout/stderr log(s).\n",
        "# You can either list paths explicitly OR use globs.\n",
        "\n",
        "# Option A: explicit paths\n",
        "EXPLICIT_LOG_PATHS: list[str] = [\n",
        "    # \"path/to/your/run.log\",\n",
        "]\n",
        "\n",
        "# Option B: auto-discover common log locations\n",
        "LOG_GLOBS: list[str] = [\n",
        "    \"**/*.log\",\n",
        "    \"**/*.out\",\n",
        "    \"**/output.log\",\n",
        "    \"**/wandb-history.jsonl\",\n",
        "]\n",
        "\n",
        "# Paths to ignore (add your big artifact dirs here if needed)\n",
        "IGNORE_SUBSTRINGS = [\n",
        "    os.sep + \".git\" + os.sep,\n",
        "    os.sep + \".venv\" + os.sep,\n",
        "]\n",
        "\n",
        "\n",
        "def discover_log_paths() -> list[str]:\n",
        "    found: list[str] = []\n",
        "    for g in LOG_GLOBS:\n",
        "        found.extend(glob(g, recursive=True))\n",
        "    # De-dupe while preserving order\n",
        "    uniq: list[str] = []\n",
        "    seen = set()\n",
        "    for p in found:\n",
        "        ap = str(Path(p))\n",
        "        if ap in seen:\n",
        "            continue\n",
        "        if any(s in ap for s in IGNORE_SUBSTRINGS):\n",
        "            continue\n",
        "        if os.path.isdir(ap):\n",
        "            continue\n",
        "        seen.add(ap)\n",
        "        uniq.append(ap)\n",
        "    return uniq\n",
        "\n",
        "\n",
        "log_paths = EXPLICIT_LOG_PATHS or discover_log_paths()\n",
        "print(f\"Found {len(log_paths)} candidate log file(s)\")\n",
        "for p in log_paths[:30]:\n",
        "    print(\"-\", p)\n",
        "if len(log_paths) > 30:\n",
        "    print(f\"... and {len(log_paths) - 30} more\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "records = parse_risk_fail_from_logs(log_paths)\n",
        "total = len(records)\n",
        "\n",
        "if total == 0:\n",
        "    raise RuntimeError(\n",
        "        \"No 'risk/fail' entries found. Set EXPLICIT_LOG_PATHS to your run log, or adjust LOG_GLOBS.\"\n",
        "    )\n",
        "\n",
        "count0 = sum(1 for r in records if r.value == 0)\n",
        "count1 = sum(1 for r in records if r.value == 1)\n",
        "count_other = total - count0 - count1\n",
        "\n",
        "failed = sum(1 for r in records if r.value == FAIL_VALUE)\n",
        "fail_rate = failed / total\n",
        "\n",
        "print(f\"Total risk/fail entries: {total}\")\n",
        "print(f\"risk/fail == 0: {count0} ({count0/total:.2%})\")\n",
        "print(f\"risk/fail == 1: {count1} ({count1/total:.2%})\")\n",
        "if count_other:\n",
        "    print(f\"risk/fail other: {count_other} ({count_other/total:.2%})\")\n",
        "print(f\"Fail rate (treating risk/fail == {FAIL_VALUE} as failure): {fail_rate:.2%}\")\n",
        "\n",
        "print(\"\\nMost recent matches:\")\n",
        "for r in records[-5:]:\n",
        "    print(f\"- {r.path}:{r.line_no}  risk/fail={r.value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: per-file breakdown (handy for multi-dataset runs saved to separate logs)\n",
        "from collections import defaultdict\n",
        "\n",
        "by_path: dict[str, list[int]] = defaultdict(list)\n",
        "for r in records:\n",
        "    by_path[r.path].append(r.value)\n",
        "\n",
        "rows = []\n",
        "for p, vals in by_path.items():\n",
        "    t = len(vals)\n",
        "    f = sum(1 for v in vals if v == FAIL_VALUE)\n",
        "    rows.append((f / t, f, t, p))\n",
        "\n",
        "rows.sort(reverse=True)\n",
        "print(f\"Per-file fail rate (FAIL_VALUE={FAIL_VALUE}):\")\n",
        "for rate, f, t, p in rows[:50]:\n",
        "    print(f\"- {rate:.2%}  ({f}/{t})  {p}\")\n",
        "if len(rows) > 50:\n",
        "    print(f\"... and {len(rows) - 50} more\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4247c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "\n",
        "def rolling_mean(xs: list[float], window: int) -> list[float]:\n",
        "    if window <= 1:\n",
        "        return xs\n",
        "    out: list[float] = []\n",
        "    s = 0.0\n",
        "    for i, x in enumerate(xs):\n",
        "        s += x\n",
        "        if i >= window:\n",
        "            s -= xs[i - window]\n",
        "        denom = min(i + 1, window)\n",
        "        out.append(s / denom)\n",
        "    return out\n",
        "\n",
        "\n",
        "# 1) Rolling fail rate over the sequence of log matches\n",
        "y = [1.0 if r.value == FAIL_VALUE else 0.0 for r in records]\n",
        "x = list(range(1, len(y) + 1))\n",
        "WINDOW = 50  # adjust for smoothing\n",
        "y_roll = rolling_mean(y, WINDOW)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(x, y_roll, linewidth=2)\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.title(f\"Rolling fail rate (window={WINDOW}, FAIL_VALUE={FAIL_VALUE})\")\n",
        "plt.xlabel(\"risk/fail match index (as encountered in logs)\")\n",
        "plt.ylabel(\"fail rate\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 2) Per-file fail rate bar chart\n",
        "paths = [p for (_, _, _, p) in rows]\n",
        "rates = [rate for (rate, _, _, _) in rows]\n",
        "\n",
        "TOP_N = 20\n",
        "paths_top = paths[:TOP_N][::-1]\n",
        "rates_top = rates[:TOP_N][::-1]\n",
        "\n",
        "plt.figure(figsize=(10, max(3, 0.35 * len(paths_top))))\n",
        "plt.barh(paths_top, rates_top)\n",
        "plt.xlim(0, 1)\n",
        "plt.title(f\"Fail rate by log file (top {min(TOP_N, len(rows))}, FAIL_VALUE={FAIL_VALUE})\")\n",
        "plt.xlabel(\"fail rate\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect DPO label building, masking, and tensor specs (from training code)\n",
    "\n",
    "Goal: reproduce the **exact** label/mask building used in training (TRL tokenization + collator + our trainerâ€™s `_concatenate_and_build_labels`) and dump **raw records** (IDs/masks/labels + token strings) for inspection.\n",
    "\n",
    "This notebook does **not** run training. It only builds one (or a few) batches and inspects them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e991410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo_root: /home/feng/github/dynamic-dpo-v1\n",
      "torch: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "# 0) Imports + repo path setup\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "repo_root = Path.cwd().resolve()\n",
    "if not (repo_root / \"src\").exists() and (repo_root.parent / \"src\").exists():\n",
    "    repo_root = repo_root.parent\n",
    "assert (repo_root / \"src\").exists(), f\"Could not find repo root from cwd={Path.cwd()}\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"repo_root:\", repo_root)\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4c1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_name: meta-llama/Llama-3.2-1B-Instruct\n",
      "ref_name: meta-llama/Llama-3.2-1B-Instruct\n",
      "pad_token_id: 128009 eos_token_id: 128009\n"
     ]
    }
   ],
   "source": [
    "# 1) Load config + tokenizer (same setup as src/cli.py)\n",
    "#\n",
    "# NOTE: For large models, make sure they are cached or you are logged into HF.\n",
    "\n",
    "from src.config.loader import load_yaml\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "CONFIG_PATH = repo_root / \"config_dpo.yaml\"\n",
    "config = load_yaml(str(CONFIG_PATH))\n",
    "\n",
    "# Use the Llama policy from config; avoid gpt2 in this notebook.\n",
    "POLICY_NAME = config.get(\"policy_name\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "REF_NAME = config.get(\"ref_name\", POLICY_NAME)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(POLICY_NAME, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "print(\"policy_name:\", POLICY_NAME)\n",
    "print(\"ref_name:\", REF_NAME)\n",
    "print(\"pad_token_id:\", tok.pad_token_id, \"eos_token_id:\", tok.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa76e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build a tiny preference dataset (prompt/chosen/rejected)\n",
    "#\n",
    "# By default we use a tiny in-memory dataset so you can inspect mechanics without HF dataset downloads.\n",
    "# Flip USE_HF_DATASET=True to load the real HH dataset (requires network/cached dataset).\n",
    "\n",
    "USE_HF_DATASET = False\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"prompt\": \"\\n\\nHuman: Give me a one-sentence tip to focus at work.\\n\\nAssistant:\",\n",
    "        \"chosen\": \" Try the Pomodoro method: 25 minutes focused, 5 minutes break.\",\n",
    "        \"rejected\": \" You should never take breaks; just work nonstop.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"\\n\\nHuman: What is 2+2?\\n\\nAssistant:\",\n",
    "        \"chosen\": \" 4.\",\n",
    "        \"rejected\": \" 22.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "if USE_HF_DATASET:\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    dataset_cfg = config.get(\"dataset\", {})\n",
    "    raw_ds = load_dataset(dataset_cfg[\"dataset_name\"], split=dataset_cfg.get(\"subset\", \"train[:1%]\"))\n",
    "\n",
    "    from src.data.hh_dataset import build_HH_dataset, apply_chat_template_to_dataset\n",
    "\n",
    "    if bool(dataset_cfg.get(\"generated_data\", False)):\n",
    "        from src.data.hh_dataset import load_generated_dataset_from_config\n",
    "\n",
    "        hh_ds = load_generated_dataset_from_config(config)\n",
    "    else:\n",
    "        hh_ds = build_HH_dataset(raw_ds)\n",
    "\n",
    "    if bool(dataset_cfg.get(\"chat_template\", False)):\n",
    "        hh_ds = apply_chat_template_to_dataset(hh_ds, tok)\n",
    "\n",
    "    rows = [hh_ds[i] for i in range(min(4, len(hh_ds)))]\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_list(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec27ebdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 3) Build the DynamicBetaDPOTrainer (same pipeline as CLI)\n",
    "#    We won't train; we only use its dataloader and collator.\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from trl import DPOConfig\n",
    "from src.trainers.dynamic_beta_dpo import DynamicBetaDPOConfig, DynamicBetaDPOTrainer\n",
    "\n",
    "# Split train/val like CLI (for completeness)\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_cfg = config.get(\"dataset\", {})\n",
    "val_ratio = float(dataset_cfg.get(\"val_ratio\", 0.1))\n",
    "seed = int(dataset_cfg.get(\"seed\", 0))\n",
    "\n",
    "split = ds.train_test_split(test_size=val_ratio, seed=seed)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds = split[\"test\"]\n",
    "\n",
    "# Model + ref model (same as CLI)\n",
    "policy = AutoModelForCausalLM.from_pretrained(POLICY_NAME)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(REF_NAME)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# Training args (mirror CLI defaults)\n",
    "prec = str(config.get(\"precision\", \"fp16\")).lower()\n",
    "fp16 = prec == \"fp16\"\n",
    "bf16 = prec == \"bf16\"\n",
    "\n",
    "train_cfg = config.get(\"dpo_training\", {})\n",
    "training_args = DPOConfig(\n",
    "    learning_rate=float(train_cfg.get(\"learning_rate\", 5e-6)),\n",
    "    per_device_train_batch_size=int(train_cfg.get(\"batch_size\", 2)),\n",
    "    per_device_eval_batch_size=int(train_cfg.get(\"eval_batch_size\", 2)),\n",
    "    num_train_epochs=int(train_cfg.get(\"epochs\", 1)),\n",
    "    logging_steps=int(train_cfg.get(\"log_steps\", 10)),\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=int(train_cfg.get(\"eval_steps\", 50)),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=int(train_cfg.get(\"save_steps\", 50)),\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    gradient_accumulation_steps=int(train_cfg.get(\"gradient_accumulation\", 1)),\n",
    "    max_grad_norm=float(train_cfg.get(\"max_grad_norm\", 1.0)),\n",
    "    warmup_steps=int(train_cfg.get(\"warmup_steps\", 0)),\n",
    "    report_to=[\"wandb\"] if train_cfg.get(\"report\") else [],\n",
    "    run_name=str(train_cfg.get(\"run_name\", \"debug\")),\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=str(train_cfg.get(\"save_dir\", \"trl_dynamic_beta_dpo\")),\n",
    "    # Truncation settings\n",
    "    max_prompt_length=int(dataset_cfg.get(\"max_prompt_length\", 256)),\n",
    "    max_completion_length=int(dataset_cfg.get(\"max_completion_length\", 256)),\n",
    "    max_length=int(dataset_cfg.get(\"max_length\", 1024)),\n",
    "    truncation_mode=str(dataset_cfg.get(\"truncation_mode\", \"keep_end\")),\n",
    ")\n",
    "\n",
    "# Dynamic-beta config\n",
    "risk = config.get(\"risk_test\", {})\n",
    "beta_up = config.get(\"beta_update\", {})\n",
    "margin_log = config.get(\"margin_log\", {})\n",
    "\n",
    "dyn_cfg = DynamicBetaDPOConfig(\n",
    "    delta=float(risk.get(\"delta\", 0.1)),\n",
    "    momentum=float(risk.get(\"lambda\", 0.05)),\n",
    "    warmup_steps=int(risk.get(\"beta_warmup\", 120)),\n",
    "    beta_0=float(beta_up.get(\"beta_0\", 0.1)),\n",
    "    alpha=float(beta_up.get(\"alpha\", 0.005)),\n",
    "    gamma=float(beta_up.get(\"gamma\", 2.0)),\n",
    "    beta_min=float(beta_up.get(\"beta_min\", 0.0)),\n",
    "    beta_max=float(beta_up.get(\"beta_max\", 2.0)),\n",
    "    log_margins=bool(margin_log.get(\"log_margins\", True)),\n",
    "    log_dir=str(margin_log.get(\"log_dir\", \"logs/margins\")),\n",
    "    jsonl_sample_size=int(margin_log.get(\"jsonl_sample_size\", 32)),\n",
    "    save_per_rank=bool(margin_log.get(\"save_per_rank\", False)),\n",
    ")\n",
    "\n",
    "trainer = DynamicBetaDPOTrainer(\n",
    "    model=policy,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    dynamic_cfg=dyn_cfg,\n",
    "    processing_class=tok,\n",
    ")\n",
    "\n",
    "print(\"trainer ready; train_dataset size:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6964302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        prompt_input_ids shape= (2, 17) dtype= torch.int64\n",
      "   prompt_attention_mask shape= (2, 17) dtype= torch.int64\n",
      "        chosen_input_ids shape= (2, 18) dtype= torch.int64\n",
      "   chosen_attention_mask shape= (2, 18) dtype= torch.int64\n",
      "      rejected_input_ids shape= (2, 12) dtype= torch.int64\n",
      " rejected_attention_mask shape= (2, 12) dtype= torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt_input_ids', 'prompt_attention_mask', 'chosen_input_ids', 'chosen_attention_mask', 'rejected_input_ids', 'rejected_attention_mask'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Pull a batch from the trainer dataloader\n",
    "\n",
    "train_loader = trainer.get_train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"{k:>24s}\", \"shape=\", tuple(v.shape), \"dtype=\", v.dtype)\n",
    "\n",
    "batch.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25692b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen_input_ids torch.Size([2, 35])\n",
      "chosen_labels torch.Size([2, 35])\n",
      "rejected_input_ids torch.Size([2, 29])\n",
      "rejected_labels torch.Size([2, 29])\n"
     ]
    }
   ],
   "source": [
    "# 5) Build concatenated tensors + labels using the trainer method\n",
    "#    (prompt tokens masked to -100; padding masked to -100)\n",
    "\n",
    "chosen_input_ids, chosen_attention_mask, chosen_labels = trainer._concatenate_and_build_labels(\n",
    "    prompt_input_ids=batch[\"prompt_input_ids\"],\n",
    "    prompt_attention_mask=batch[\"prompt_attention_mask\"],\n",
    "    completion_input_ids=batch[\"chosen_input_ids\"],\n",
    "    completion_attention_mask=batch[\"chosen_attention_mask\"],\n",
    ")\n",
    "\n",
    "rejected_input_ids, rejected_attention_mask, rejected_labels = trainer._concatenate_and_build_labels(\n",
    "    prompt_input_ids=batch[\"prompt_input_ids\"],\n",
    "    prompt_attention_mask=batch[\"prompt_attention_mask\"],\n",
    "    completion_input_ids=batch[\"rejected_input_ids\"],\n",
    "    completion_attention_mask=batch[\"rejected_attention_mask\"],\n",
    ")\n",
    "\n",
    "print(\"chosen_input_ids\", chosen_input_ids.shape)\n",
    "print(\"chosen_labels\", chosen_labels.shape)\n",
    "print(\"rejected_input_ids\", rejected_input_ids.shape)\n",
    "print(\"rejected_labels\", rejected_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# 6) Invariants: prompt is masked; padding is masked\n",
    "\n",
    "def assert_label_invariants(\n",
    "    prompt_input_ids: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    concat_attention_mask: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    ") -> None:\n",
    "    prompt_len = prompt_input_ids.shape[1]\n",
    "    assert (labels[:, :prompt_len] == -100).all().item(), \"Prompt tokens should be all -100\"\n",
    "    assert (labels[concat_attention_mask == 0] == -100).all().item(), \"Padding tokens should be -100\"\n",
    "    # Completion: any non-padding completion token should be unmasked\n",
    "    completion_mask = concat_attention_mask.clone()\n",
    "    completion_mask[:, :prompt_len] = 0\n",
    "    if completion_mask.any().item():\n",
    "        assert (labels[completion_mask == 1] != -100).all().item(), \"Completion tokens should be unmasked\"\n",
    "\n",
    "\n",
    "assert_label_invariants(batch[\"prompt_input_ids\"], batch[\"prompt_attention_mask\"], chosen_attention_mask, chosen_labels)\n",
    "assert_label_invariants(batch[\"prompt_input_ids\"], batch[\"prompt_attention_mask\"], rejected_attention_mask, rejected_labels)\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6470b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Raw per-token inspection (IDs, tokens, attention_mask, label mask)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _first_eos_index(ids, eos_id):\n",
    "    if eos_id is None:\n",
    "        return None\n",
    "    for i, tid in enumerate(ids):\n",
    "        if tid == eos_id:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def _tensor_to_list(value, idx):\n",
    "    if value is None:\n",
    "        return []\n",
    "    if hasattr(value, \"tolist\"):\n",
    "        if value.ndim == 1:\n",
    "            return value.tolist()\n",
    "        return value[idx].tolist()\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        if not value:\n",
    "            return []\n",
    "        first = value[0]\n",
    "        if isinstance(first, (list, tuple)):\n",
    "            return list(value[idx])\n",
    "        return list(value)\n",
    "    return []\n",
    "\n",
    "\n",
    "def token_table(\n",
    "    *,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    sample_idx: int,\n",
    "    tok,\n",
    "    trim_to_first_eos: bool = True,\n",
    ") -> tuple[pd.DataFrame, list[int]]:\n",
    "    ids_full = _tensor_to_list(input_ids, sample_idx)\n",
    "    attn_full = _tensor_to_list(attention_mask, sample_idx)\n",
    "    labs_full = _tensor_to_list(labels, sample_idx)\n",
    "    toks_full = tok.convert_ids_to_tokens(ids_full)\n",
    "\n",
    "    eos_id = tok.eos_token_id\n",
    "    eos_positions = [i for i, tid in enumerate(ids_full) if tid == eos_id] if eos_id is not None else []\n",
    "    first_eos = _first_eos_index(ids_full, eos_id)\n",
    "\n",
    "    if trim_to_first_eos and first_eos is not None:\n",
    "        ids = ids_full[: first_eos + 1]\n",
    "        attn = attn_full[: first_eos + 1]\n",
    "        labs = labs_full[: first_eos + 1]\n",
    "        toks = toks_full[: first_eos + 1]\n",
    "    else:\n",
    "        ids, attn, labs, toks = ids_full, attn_full, labs_full, toks_full\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"pos\": list(range(len(ids))),\n",
    "            \"token_id\": ids,\n",
    "            \"token\": toks,\n",
    "            \"attn\": attn,\n",
    "            \"label\": labs,\n",
    "            \"label_masked\": [x == -100 for x in labs],\n",
    "            \"is_eos\": [x == eos_id for x in ids] if eos_id is not None else [False] * len(ids),\n",
    "        }\n",
    "    )\n",
    "    return df, eos_positions\n",
    "\n",
    "\n",
    "SAMPLE_IDX = 0\n",
    "\n",
    "print(\"RAW TEXT\")\n",
    "print(\"prompt:\", rows[SAMPLE_IDX][\"prompt\"])\n",
    "print(\"chosen:\", rows[SAMPLE_IDX][\"chosen\"])\n",
    "print(\"rejected:\", rows[SAMPLE_IDX][\"rejected\"])\n",
    "\n",
    "chosen_df, chosen_eos_positions = token_table(\n",
    "    input_ids=chosen_input_ids,\n",
    "    attention_mask=chosen_attention_mask,\n",
    "    labels=chosen_labels,\n",
    "    sample_idx=SAMPLE_IDX,\n",
    "    tok=tok,\n",
    ")\n",
    "rejected_df, rejected_eos_positions = token_table(\n",
    "    input_ids=rejected_input_ids,\n",
    "    attention_mask=rejected_attention_mask,\n",
    "    labels=rejected_labels,\n",
    "    sample_idx=SAMPLE_IDX,\n",
    "    tok=tok,\n",
    ")\n",
    "\n",
    "print(\"eos_token:\", tok.eos_token, \"eos_token_id:\", tok.eos_token_id)\n",
    "print(\"chosen eos positions (full):\", chosen_eos_positions)\n",
    "print(\"rejected eos positions (full):\", rejected_eos_positions)\n",
    "\n",
    "display(chosen_df)\n",
    "display(rejected_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe8cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen_loss_mask shape: torch.Size([2, 34]) valid tokens: 22\n",
      "rejected_loss_mask shape: torch.Size([2, 28]) valid tokens: 16\n"
     ]
    }
   ],
   "source": [
    "# 8) Show what compute_log_prob will consider \"valid\" (labels shifted by 1)\n",
    "\n",
    "from src.losses.dpo_loss import compute_log_prob\n",
    "\n",
    "\n",
    "def build_loss_mask(labels: torch.Tensor) -> torch.Tensor:\n",
    "    # compute_log_prob() uses labels[:, 1:] (shift by 1)\n",
    "    shifted = labels[:, 1:].clone()\n",
    "    return (shifted != -100)\n",
    "\n",
    "\n",
    "chosen_loss_mask = build_loss_mask(chosen_labels)\n",
    "rejected_loss_mask = build_loss_mask(rejected_labels)\n",
    "\n",
    "print(\"chosen_loss_mask shape:\", chosen_loss_mask.shape, \"valid tokens:\", int(chosen_loss_mask.sum().item()))\n",
    "print(\"rejected_loss_mask shape:\", rejected_loss_mask.shape, \"valid tokens:\", int(rejected_loss_mask.sum().item()))\n",
    "\n",
    "# (Optional) You need logits to actually run compute_log_prob; this cell just mirrors its masking logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: /tmp/dpo_label_mask_debug.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 9) Dump raw records (IDs/masks/labels + token strings) to JSONL\n",
    "#\n",
    "# Default output is under /tmp so we don't create repo artifacts.\n",
    "\n",
    "\n",
    "def dump_records_jsonl(path: Path, *, rows):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, r in enumerate(rows):\n",
    "            prompt_padded_len = int(batch[\"prompt_input_ids\"].shape[1])\n",
    "            chosen_comp_padded_len = int(batch[\"chosen_input_ids\"].shape[1])\n",
    "            rejected_comp_padded_len = int(batch[\"rejected_input_ids\"].shape[1])\n",
    "            chosen_concat_padded_len = int(chosen_input_ids.shape[1])\n",
    "            rejected_concat_padded_len = int(rejected_input_ids.shape[1])\n",
    "\n",
    "            prompt_actual_len = int(batch[\"prompt_attention_mask\"][i].sum().item())\n",
    "            chosen_comp_actual_len = int(batch[\"chosen_attention_mask\"][i].sum().item())\n",
    "            rejected_comp_actual_len = int(batch[\"rejected_attention_mask\"][i].sum().item())\n",
    "            chosen_concat_actual_len = int(chosen_attention_mask[i].sum().item())\n",
    "            rejected_concat_actual_len = int(rejected_attention_mask[i].sum().item())\n",
    "\n",
    "            chosen_valid_tokens = int((chosen_labels[i] != -100).sum().item())\n",
    "            rejected_valid_tokens = int((rejected_labels[i] != -100).sum().item())\n",
    "            chosen_valid_tokens_shifted = int((chosen_labels[i, 1:] != -100).sum().item())\n",
    "            rejected_valid_tokens_shifted = int((rejected_labels[i, 1:] != -100).sum().item())\n",
    "\n",
    "            rec = {\n",
    "                \"idx\": i,\n",
    "                \"raw\": {\"prompt\": r[\"prompt\"], \"chosen\": r[\"chosen\"], \"rejected\": r[\"rejected\"]},\n",
    "                \"lengths\": {\n",
    "                    \"prompt_padded_len\": prompt_padded_len,\n",
    "                    \"prompt_actual_len\": prompt_actual_len,\n",
    "                    \"chosen_completion_padded_len\": chosen_comp_padded_len,\n",
    "                    \"chosen_completion_actual_len\": chosen_comp_actual_len,\n",
    "                    \"rejected_completion_padded_len\": rejected_comp_padded_len,\n",
    "                    \"rejected_completion_actual_len\": rejected_comp_actual_len,\n",
    "                    \"chosen_concat_padded_len\": chosen_concat_padded_len,\n",
    "                    \"chosen_concat_actual_len\": chosen_concat_actual_len,\n",
    "                    \"rejected_concat_padded_len\": rejected_concat_padded_len,\n",
    "                    \"rejected_concat_actual_len\": rejected_concat_actual_len,\n",
    "                    \"chosen_valid_tokens\": chosen_valid_tokens,\n",
    "                    \"rejected_valid_tokens\": rejected_valid_tokens,\n",
    "                    \"chosen_valid_tokens_shifted\": chosen_valid_tokens_shifted,\n",
    "                    \"rejected_valid_tokens_shifted\": rejected_valid_tokens_shifted,\n",
    "                },\n",
    "                \"prompt\": {\n",
    "                    \"input_ids\": batch[\"prompt_input_ids\"][i].tolist(),\n",
    "                    \"attention_mask\": batch[\"prompt_attention_mask\"][i].tolist(),\n",
    "                },\n",
    "                \"chosen_completion\": {\n",
    "                    \"input_ids\": batch[\"chosen_input_ids\"][i].tolist(),\n",
    "                    \"attention_mask\": batch[\"chosen_attention_mask\"][i].tolist(),\n",
    "                },\n",
    "                \"rejected_completion\": {\n",
    "                    \"input_ids\": batch[\"rejected_input_ids\"][i].tolist(),\n",
    "                    \"attention_mask\": batch[\"rejected_attention_mask\"][i].tolist(),\n",
    "                },\n",
    "                \"chosen_concat\": {\n",
    "                    \"input_ids\": chosen_input_ids[i].tolist(),\n",
    "                    \"attention_mask\": chosen_attention_mask[i].tolist(),\n",
    "                    \"labels\": chosen_labels[i].tolist(),\n",
    "                    \"tokens\": tok.convert_ids_to_tokens(chosen_input_ids[i].tolist()),\n",
    "                },\n",
    "                \"rejected_concat\": {\n",
    "                    \"input_ids\": rejected_input_ids[i].tolist(),\n",
    "                    \"attention_mask\": rejected_attention_mask[i].tolist(),\n",
    "                    \"labels\": rejected_labels[i].tolist(),\n",
    "                    \"tokens\": tok.convert_ids_to_tokens(rejected_input_ids[i].tolist()),\n",
    "                },\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "OUT_PATH = Path(\"/tmp/dpo_label_mask_debug.jsonl\")\n",
    "dump_records_jsonl(OUT_PATH, rows=rows)\n",
    "print(\"wrote:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84906ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-dpo-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inspect DPO label building, masking, and tensor specs (from training code)\n",
        "\n",
        "Goal: reproduce the **exact** label/mask building used in training (TRL tokenization + collator + our trainerâ€™s `_concatenate_and_build_labels`) and dump **raw records** (IDs/masks/labels + token strings) for inspection.\n",
        "\n",
        "This notebook does **not** run training. It only builds one (or a few) batches and inspects them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Imports + repo path setup\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "repo_root = Path.cwd().resolve()\n",
        "if not (repo_root / \"src\").exists() and (repo_root.parent / \"src\").exists():\n",
        "    repo_root = repo_root.parent\n",
        "assert (repo_root / \"src\").exists(), f\"Could not find repo root from cwd={Path.cwd()}\"\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, str(repo_root))\n",
        "\n",
        "print(\"repo_root:\", repo_root)\n",
        "print(\"torch:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Load config (optional) + pick model/tokenizer\n",
        "#\n",
        "# NOTE: For Llama models, you may need HF auth + local cache.\n",
        "# If you only want to inspect token/label mechanics without downloading large models,\n",
        "# set MODEL_NAME to something small that is already cached.\n",
        "\n",
        "from src.config.loader import load_yaml\n",
        "\n",
        "CONFIG_PATH = repo_root / \"config_dpo.yaml\"\n",
        "config = load_yaml(str(CONFIG_PATH))\n",
        "\n",
        "MODEL_NAME = config.get(\"policy_name\", \"gpt2\")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "print(\"MODEL_NAME:\", MODEL_NAME)\n",
        "print(\"pad_token_id:\", tok.pad_token_id)\n",
        "print(\"eos_token_id:\", tok.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Build a tiny preference dataset (prompt/chosen/rejected)\n",
        "#\n",
        "# By default we use a tiny in-memory dataset so you can inspect mechanics without HF dataset downloads.\n",
        "# Flip USE_HF_DATASET=True to load the real HH dataset (requires network/cached dataset).\n",
        "\n",
        "USE_HF_DATASET = False\n",
        "\n",
        "rows = [\n",
        "    {\n",
        "        \"prompt\": \"\\n\\nHuman: Give me a one-sentence tip to focus at work.\\n\\nAssistant:\",\n",
        "        \"chosen\": \" Try the Pomodoro method: 25 minutes focused, 5 minutes break.\",\n",
        "        \"rejected\": \" You should never take breaks; just work nonstop.\",\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"\\n\\nHuman: What is 2+2?\\n\\nAssistant:\",\n",
        "        \"chosen\": \" 4.\",\n",
        "        \"rejected\": \" 22.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "if USE_HF_DATASET:\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    dataset_cfg = config.get(\"dataset\", {})\n",
        "    raw_ds = load_dataset(dataset_cfg[\"dataset_name\"], split=dataset_cfg.get(\"subset\", \"train[:1%]\"))\n",
        "\n",
        "    from src.data.hh_dataset import build_HH_dataset, apply_chat_template_to_dataset\n",
        "\n",
        "    if bool(dataset_cfg.get(\"generated_data\", False)):\n",
        "        from src.data.hh_dataset import load_generated_dataset_from_config\n",
        "\n",
        "        hh_ds = load_generated_dataset_from_config(config)\n",
        "    else:\n",
        "        hh_ds = build_HH_dataset(raw_ds)\n",
        "\n",
        "    if bool(dataset_cfg.get(\"chat_template\", False)):\n",
        "        hh_ds = apply_chat_template_to_dataset(hh_ds, tok)\n",
        "\n",
        "    rows = [hh_ds[i] for i in range(min(4, len(hh_ds)))]\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_list(rows)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Tokenize exactly like TRL DPOTrainer.tokenize_row (prompt/chosen/rejected separated)\n",
        "\n",
        "from trl.trainer.dpo_trainer import DPOTrainer\n",
        "\n",
        "max_prompt_length = int(config.get(\"dataset\", {}).get(\"max_prompt_length\", 256))\n",
        "max_completion_length = int(config.get(\"dataset\", {}).get(\"max_completion_length\", 256))\n",
        "\n",
        "\n",
        "def tokenize_row(row: Dict[str, str]) -> Dict[str, List[int]]:\n",
        "    return DPOTrainer.tokenize_row(\n",
        "        row,\n",
        "        processing_class=tok,\n",
        "        max_prompt_length=max_prompt_length,\n",
        "        max_completion_length=max_completion_length,\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "\n",
        "\n",
        "tok_rows = [tokenize_row(r) for r in rows]\n",
        "tok_rows[0].keys(), {k: len(v) for k, v in tok_rows[0].items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Collate exactly like TRL's DataCollatorForPreference (prompt left-pad, completions right-pad)\n",
        "\n",
        "from trl.trainer.dpo_trainer import DataCollatorForPreference\n",
        "\n",
        "collator = DataCollatorForPreference(pad_token_id=int(tok.pad_token_id))\n",
        "batch = collator(tok_rows)\n",
        "\n",
        "for k, v in batch.items():\n",
        "    if hasattr(v, \"shape\"):\n",
        "        print(f\"{k:>24s}\", \"shape=\", tuple(v.shape), \"dtype=\", v.dtype)\n",
        "\n",
        "batch.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Build concatenated tensors + labels using the *training* method\n",
        "#    (prompt tokens masked to -100; padding masked to -100)\n",
        "\n",
        "from src.trainers.dynamic_beta_dpo import DynamicBetaDPOTrainer\n",
        "\n",
        "chosen_input_ids, chosen_attention_mask, chosen_labels = DynamicBetaDPOTrainer._concatenate_and_build_labels(\n",
        "    None,\n",
        "    prompt_input_ids=batch[\"prompt_input_ids\"],\n",
        "    prompt_attention_mask=batch[\"prompt_attention_mask\"],\n",
        "    completion_input_ids=batch[\"chosen_input_ids\"],\n",
        "    completion_attention_mask=batch[\"chosen_attention_mask\"],\n",
        ")\n",
        "\n",
        "rejected_input_ids, rejected_attention_mask, rejected_labels = DynamicBetaDPOTrainer._concatenate_and_build_labels(\n",
        "    None,\n",
        "    prompt_input_ids=batch[\"prompt_input_ids\"],\n",
        "    prompt_attention_mask=batch[\"prompt_attention_mask\"],\n",
        "    completion_input_ids=batch[\"rejected_input_ids\"],\n",
        "    completion_attention_mask=batch[\"rejected_attention_mask\"],\n",
        ")\n",
        "\n",
        "print(\"chosen_input_ids\", chosen_input_ids.shape)\n",
        "print(\"chosen_labels\", chosen_labels.shape)\n",
        "print(\"rejected_input_ids\", rejected_input_ids.shape)\n",
        "print(\"rejected_labels\", rejected_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Invariants: prompt is masked; padding is masked\n",
        "\n",
        "def assert_label_invariants(\n",
        "    prompt_input_ids: torch.Tensor,\n",
        "    prompt_attention_mask: torch.Tensor,\n",
        "    concat_attention_mask: torch.Tensor,\n",
        "    labels: torch.Tensor,\n",
        ") -> None:\n",
        "    prompt_len = prompt_input_ids.shape[1]\n",
        "    assert (labels[:, :prompt_len] == -100).all().item(), \"Prompt tokens should be all -100\"\n",
        "    assert (labels[concat_attention_mask == 0] == -100).all().item(), \"Padding tokens should be -100\"\n",
        "    # Completion: any non-padding completion token should be unmasked\n",
        "    completion_mask = concat_attention_mask.clone()\n",
        "    completion_mask[:, :prompt_len] = 0\n",
        "    if completion_mask.any().item():\n",
        "        assert (labels[completion_mask == 1] != -100).all().item(), \"Completion tokens should be unmasked\"\n",
        "\n",
        "\n",
        "assert_label_invariants(batch[\"prompt_input_ids\"], batch[\"prompt_attention_mask\"], chosen_attention_mask, chosen_labels)\n",
        "assert_label_invariants(batch[\"prompt_input_ids\"], batch[\"prompt_attention_mask\"], rejected_attention_mask, rejected_labels)\n",
        "print(\"OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Raw per-token inspection (IDs, tokens, attention_mask, label mask)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def token_table(\n",
        "    *,\n",
        "    input_ids: torch.Tensor,\n",
        "    attention_mask: torch.Tensor,\n",
        "    labels: torch.Tensor,\n",
        "    sample_idx: int,\n",
        "    tok,\n",
        ") -> pd.DataFrame:\n",
        "    ids = input_ids[sample_idx].tolist()\n",
        "    attn = attention_mask[sample_idx].tolist()\n",
        "    labs = labels[sample_idx].tolist()\n",
        "    toks = tok.convert_ids_to_tokens(ids)\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"pos\": list(range(len(ids))),\n",
        "            \"token_id\": ids,\n",
        "            \"token\": toks,\n",
        "            \"attn\": attn,\n",
        "            \"label\": labs,\n",
        "            \"label_masked\": [x == -100 for x in labs],\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "SAMPLE_IDX = 0\n",
        "\n",
        "print(\"RAW TEXT\")\n",
        "print(\"prompt:\", rows[SAMPLE_IDX][\"prompt\"])\n",
        "print(\"chosen:\", rows[SAMPLE_IDX][\"chosen\"])\n",
        "print(\"rejected:\", rows[SAMPLE_IDX][\"rejected\"])\n",
        "\n",
        "chosen_df = token_table(\n",
        "    input_ids=chosen_input_ids,\n",
        "    attention_mask=chosen_attention_mask,\n",
        "    labels=chosen_labels,\n",
        "    sample_idx=SAMPLE_IDX,\n",
        "    tok=tok,\n",
        ")\n",
        "rejected_df = token_table(\n",
        "    input_ids=rejected_input_ids,\n",
        "    attention_mask=rejected_attention_mask,\n",
        "    labels=rejected_labels,\n",
        "    sample_idx=SAMPLE_IDX,\n",
        "    tok=tok,\n",
        ")\n",
        "\n",
        "display(chosen_df)\n",
        "display(rejected_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Show what compute_log_prob will consider \"valid\" (labels shifted by 1)\n",
        "\n",
        "from src.losses.dpo_loss import compute_log_prob\n",
        "\n",
        "\n",
        "def build_loss_mask(labels: torch.Tensor) -> torch.Tensor:\n",
        "    # compute_log_prob() uses labels[:, 1:] (shift by 1)\n",
        "    shifted = labels[:, 1:].clone()\n",
        "    return (shifted != -100)\n",
        "\n",
        "\n",
        "chosen_loss_mask = build_loss_mask(chosen_labels)\n",
        "rejected_loss_mask = build_loss_mask(rejected_labels)\n",
        "\n",
        "print(\"chosen_loss_mask shape:\", chosen_loss_mask.shape, \"valid tokens:\", int(chosen_loss_mask.sum().item()))\n",
        "print(\"rejected_loss_mask shape:\", rejected_loss_mask.shape, \"valid tokens:\", int(rejected_loss_mask.sum().item()))\n",
        "\n",
        "# (Optional) You need logits to actually run compute_log_prob; this cell just mirrors its masking logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) Dump raw records (IDs/masks/labels + token strings) to JSONL\n",
        "#\n",
        "# Default output is under /tmp so we don't create repo artifacts.\n",
        "\n",
        "def dump_records_jsonl(path: Path, *, rows: List[Dict[str, str]]) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for i, r in enumerate(rows):\n",
        "            prompt_padded_len = int(batch[\"prompt_input_ids\"].shape[1])\n",
        "            chosen_comp_padded_len = int(batch[\"chosen_input_ids\"].shape[1])\n",
        "            rejected_comp_padded_len = int(batch[\"rejected_input_ids\"].shape[1])\n",
        "            chosen_concat_padded_len = int(chosen_input_ids.shape[1])\n",
        "            rejected_concat_padded_len = int(rejected_input_ids.shape[1])\n",
        "\n",
        "            prompt_actual_len = int(batch[\"prompt_attention_mask\"][i].sum().item())\n",
        "            chosen_comp_actual_len = int(batch[\"chosen_attention_mask\"][i].sum().item())\n",
        "            rejected_comp_actual_len = int(batch[\"rejected_attention_mask\"][i].sum().item())\n",
        "            chosen_concat_actual_len = int(chosen_attention_mask[i].sum().item())\n",
        "            rejected_concat_actual_len = int(rejected_attention_mask[i].sum().item())\n",
        "\n",
        "            chosen_valid_tokens = int((chosen_labels[i] != -100).sum().item())\n",
        "            rejected_valid_tokens = int((rejected_labels[i] != -100).sum().item())\n",
        "            chosen_valid_tokens_shifted = int((chosen_labels[i, 1:] != -100).sum().item())\n",
        "            rejected_valid_tokens_shifted = int((rejected_labels[i, 1:] != -100).sum().item())\n",
        "\n",
        "            rec = {\n",
        "                \"idx\": i,\n",
        "                \"raw\": {\"prompt\": r[\"prompt\"], \"chosen\": r[\"chosen\"], \"rejected\": r[\"rejected\"]},\n",
        "                \"lengths\": {\n",
        "                    \"prompt_padded_len\": prompt_padded_len,\n",
        "                    \"prompt_actual_len\": prompt_actual_len,\n",
        "                    \"chosen_completion_padded_len\": chosen_comp_padded_len,\n",
        "                    \"chosen_completion_actual_len\": chosen_comp_actual_len,\n",
        "                    \"rejected_completion_padded_len\": rejected_comp_padded_len,\n",
        "                    \"rejected_completion_actual_len\": rejected_comp_actual_len,\n",
        "                    \"chosen_concat_padded_len\": chosen_concat_padded_len,\n",
        "                    \"chosen_concat_actual_len\": chosen_concat_actual_len,\n",
        "                    \"rejected_concat_padded_len\": rejected_concat_padded_len,\n",
        "                    \"rejected_concat_actual_len\": rejected_concat_actual_len,\n",
        "                    \"chosen_valid_tokens\": chosen_valid_tokens,\n",
        "                    \"rejected_valid_tokens\": rejected_valid_tokens,\n",
        "                    \"chosen_valid_tokens_shifted\": chosen_valid_tokens_shifted,\n",
        "                    \"rejected_valid_tokens_shifted\": rejected_valid_tokens_shifted,\n",
        "                },\n",
        "                \"prompt\": {\n",
        "                    \"input_ids\": batch[\"prompt_input_ids\"][i].tolist(),\n",
        "                    \"attention_mask\": batch[\"prompt_attention_mask\"][i].tolist(),\n",
        "                },\n",
        "                \"chosen_completion\": {\n",
        "                    \"input_ids\": batch[\"chosen_input_ids\"][i].tolist(),\n",
        "                    \"attention_mask\": batch[\"chosen_attention_mask\"][i].tolist(),\n",
        "                },\n",
        "                \"rejected_completion\": {\n",
        "                    \"input_ids\": batch[\"rejected_input_ids\"][i].tolist(),\n",
        "                    \"attention_mask\": batch[\"rejected_attention_mask\"][i].tolist(),\n",
        "                },\n",
        "                \"chosen_concat\": {\n",
        "                    \"input_ids\": chosen_input_ids[i].tolist(),\n",
        "                    \"attention_mask\": chosen_attention_mask[i].tolist(),\n",
        "                    \"labels\": chosen_labels[i].tolist(),\n",
        "                    \"tokens\": tok.convert_ids_to_tokens(chosen_input_ids[i].tolist()),\n",
        "                },\n",
        "                \"rejected_concat\": {\n",
        "                    \"input_ids\": rejected_input_ids[i].tolist(),\n",
        "                    \"attention_mask\": rejected_attention_mask[i].tolist(),\n",
        "                    \"labels\": rejected_labels[i].tolist(),\n",
        "                    \"tokens\": tok.convert_ids_to_tokens(rejected_input_ids[i].tolist()),\n",
        "                },\n",
        "            }\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "OUT_PATH = Path(\"/tmp/dpo_label_mask_debug.jsonl\")\n",
        "dump_records_jsonl(OUT_PATH, rows=rows)\n",
        "print(\"wrote:\", OUT_PATH)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dynamic-dpo-v1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
